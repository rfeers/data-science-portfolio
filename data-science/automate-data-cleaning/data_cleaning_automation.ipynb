{"cells":[{"cell_type":"markdown","id":"a3d4387f-fa24-4a80-906b-4cca195b6736","metadata":{},"source":["# Automating Data Cleaning with a 5 steps pipeline\n","\n","**Defining a pipeline to automate the data cleaning.**\n"]},{"cell_type":"code","execution_count":1,"id":"635c6429","metadata":{},"outputs":[],"source":["import pandas as pd \n","from sklearn.preprocessing import LabelEncoder \n","import numpy as np \n","import os\n"]},{"cell_type":"markdown","id":"3bd3a35d","metadata":{},"source":["## STEP 1 - READING THE DATA"]},{"cell_type":"code","execution_count":2,"id":"0467ddbe","metadata":{},"outputs":[],"source":["# Function to read data based on file extension\n","def read_data(file_path):\n","    _ , file_ext = os.path.splitext(file_path)\n","    if file_ext == '.csv':\n","        return pd.read_csv(file_path)\n","    elif file_ext == '.json':\n","        return pd.read_json(file_path)\n","    elif file_ext in ['.xls', '.xlsx']:\n","        return pd.read_excel(file_path)\n","    else:\n","        raise ValueError(\"Unknown file format\")"]},{"cell_type":"markdown","id":"0a26a108","metadata":{},"source":["## STEP 2 - DEALING WITH DUPLICATES"]},{"cell_type":"code","execution_count":3,"id":"6f7423f3","metadata":{},"outputs":[],"source":["# 2. Check if there are duplicates\n","def drop_duplicates(df, columns=None): \n","\tif columns == None: \n","\t\tdf.drop_duplicates(inplace=True) \n","\telse: \n","\t\tdf.drop_duplicates(subset = columns, inplace=False)\n","\treturn df "]},{"cell_type":"markdown","id":"f0d8d341","metadata":{},"source":["## STEP 3 - DEALING WITH MISSING VALUES"]},{"cell_type":"code","execution_count":4,"id":"afff150b","metadata":{},"outputs":[],"source":["def check_missing_data(df):\n","    # Check for missing values\n","    proportion_null_rows = 100*(round(df.isnull().any(axis=1).sum()/df.any(axis=1).count(),2))\n","    if proportion_null_rows <= 5:\n","        print(f\"There are {df.isnull().any(axis=1).sum()} rows with a null value. All of them are erased!\")\n","        df.dropna()\n","    else:\n","        print(\"Too many null values, we need to check columns by columns further.\")\n","        if df.isnull().sum().sum() > 0:\n","            print(\"\\nProportion of missing values by column\")\n","            values = 100*(round(df.isnull().sum()/df.count(),2))\n","            print(values)\n","            dealing_missing_data(df)\n","        else:\n","            print(\"No missing values detected!\")\n","            \n","\n","def dealing_missing_data(df):\n","    values = 100*(round(df.isnull().sum()/df.count(),2))\n","    to_delete = []\n","    to_impute = []\n","    to_check = []\n","    for name, proportion in values.items():\n","        if int(proportion) == 0:\n","            continue\n","        elif int(proportion) <= 10:\n","            to_impute.append(name)\n","            df.fillna(df[name].median()) \n","        else: \n","            to_check.append(name)\n","    print(f\"\\nThe missing values in {to_impute} have been replaced by the median.\")\n","    print(f\"The columns {to_check} should be further understood\")\n","    \n"]},{"cell_type":"markdown","id":"29fb06d3","metadata":{},"source":["## STEP 4 - DETECTING DATA TYPES MISMATCHES"]},{"cell_type":"code","execution_count":5,"id":"a83344e7","metadata":{},"outputs":[],"source":["# define the expected types\n","expected_types = {'recipe': 'int64', \n","                  'calories': 'float64', \n","                  'carbohydrate': 'float64',\n","                  'sugar': 'float64', \n","                  'protein': 'float64',\n","                  'category': 'str', \n","                  'servings': 'int64',\n","                  'high_traffic': 'bool'               \n","                  }\n","\n","# detect type mismatches\n","def check_data_types(df, expected_types):\n","    \"\"\"\n","    Check the data types of a DataFrame against expected types.\n","\n","    Parameters:\n","    - df (pd.DataFrame): The DataFrame to check.\n","    - expected_types (dict): A dictionary mapping column names to expected data types (e.g., 'int', 'float', 'datetime').\n","\n","    Returns:\n","    - dict: A report of mismatches and suggested corrections.\n","    \"\"\"\n","    for column, expected_type in expected_types.items():\n","        actual_type = df[column].dtype\n","\n","        # Create a readable version of numpy dtype for reporting\n","        readable_type = np.dtype(actual_type).name\n","        if not np.issubdtype(actual_type, np.dtype(expected_type).type):\n","            message = f\"Column '{column}' has type '{readable_type}' instead of '{expected_type}'.\"\n","            suggestion = f\"Convert '{column}' to '{expected_type}'.\"\n","            print(f\"{message}\", f\"{suggestion}\")\n","\n","    print(\"No data types mismatch detected\")"]},{"cell_type":"markdown","id":"1e2ba7d0","metadata":{},"source":["## STEP 5 - DETECTING OUTLIERS"]},{"cell_type":"code","execution_count":6,"id":"9247a0dc","metadata":{},"outputs":[],"source":["# Function to find outliers using IQR\n","def find_outliers_IQR(df):\n","    outlier_indices = []\n","    df = df.select_dtypes(include=['number'])\n","    for column in df.columns:\n","        Q1 = df[column].quantile(0.25)\n","        Q3 = df[column].quantile(0.75)\n","        IQR = Q3 - Q1\n","        lower_bound = Q1 - 1.5 * IQR\n","        upper_bound = Q3 + 1.5 * IQR\n","        \n","        # Get the indices of outliers for feature column\n","        outlier_list_col = df[(df[column] < lower_bound) | (df[column] > upper_bound)].index\n","        outlier_indices.extend(outlier_list_col)\n","    \n","    outlier_indices = list(set(outlier_indices))  # Get unique indices\n","    return df.iloc[outlier_indices]\n"]},{"cell_type":"markdown","id":"9f526329","metadata":{},"source":["# FINAL PIPELINE"]},{"cell_type":"code","execution_count":7,"id":"2533958e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Too many null values, we need to check columns by columns further.\n","\n","Proportion of missing values by column\n","recipe           0.0\n","calories         6.0\n","carbohydrate     6.0\n","sugar            6.0\n","protein          6.0\n","category         0.0\n","servings         0.0\n","high_traffic    65.0\n","dtype: float64\n","\n","The missing values in ['calories', 'carbohydrate', 'sugar', 'protein'] have been replaced by the median.\n","The columns ['high_traffic'] should be further understood\n","Column 'category' has type 'object' instead of 'str'. Convert 'category' to 'str'.\n","Column 'servings' has type 'object' instead of 'int64'. Convert 'servings' to 'int64'.\n","Column 'high_traffic' has type 'object' instead of 'bool'. Convert 'high_traffic' to 'bool'.\n","No data types mismatch detected\n","Outliers detected using IQR:\n","     recipe  calories  carbohydrate  sugar  protein\n","513     514      2.98          9.81  28.58    19.11\n","3         4     97.03         30.56  38.63     0.02\n","518     519    161.81         16.80  30.56     0.14\n","520     521    363.51         23.52   0.50    78.49\n","521     522    600.99        195.80   5.25    23.11\n","..      ...       ...           ...    ...      ...\n","493     494    581.97          8.29  29.15     0.43\n","496     497    260.15         23.07  71.52     3.22\n","501     502    883.12         70.81   6.85    94.00\n","503     504    529.26        100.70   5.88     9.60\n","510     511    217.57          8.43   7.26    78.47\n","\n","[241 rows x 5 columns]\n"]}],"source":["file_path = \"data/my_data.csv\"\n","\n","df = read_data(\"data/my_data.csv\")\n","\n","df = drop_duplicates(df, columns=[\"recipe\",\"calories\"])\n","\n","check_missing_data(df)\n","\n","check_data_types(df, expected_types)\n","\n","outliers = find_outliers_IQR(df)\n","print(\"Outliers detected using IQR:\")\n","print(outliers)"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":5}
