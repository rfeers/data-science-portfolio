{"cells":[{"source":"<span style=\"font-size: 30px;\">**Chat with Your Documents Using GPT & LangChain**</span>\n\n\n**Objectives:** \n- *Learn how to effectively load & store documents using LangChain*\n- *Build a retrieval augmented generation pipeline for querying data*\n- *Build a question-answering bot that answers questions based on your documents*\n\nYou can learn more about the LangChain library in the following links:\n* [How to Make Large Language Models Play Nice with Your Software Using LangChain](https://www.kdnuggets.com/how-to-make-large-language-models-play-nice-with-your-software-using-langchain)\n* [6 Problems of LLMs That LangChain is Trying to Assess](https://www.kdnuggets.com/6-problems-of-llms-that-langchain-is-trying-to-assess)\n\nLet's start by understanding our main goal:\n\nFirst: \n- Take a set of PDFs. \n- Break them into pieces of texts. \n- Embed them into a vectorized representation. \n- Store them into a vector database. (FAISS, CHROMA, PINECONE...)\n- Once the vectors are persistend in the ddbb, we can get queries, embed them and find a similar chunk vectors. \n- The chunks are ranked according to how relevant they are to the question and are used to contextualize our LLM. \n\n**IMPORTANT:** The LLM doesn't really know what PDFs have. We take advantage of the LLM model to generate NLP answers and provide it with a question and a context to generate an accurate answer. \n\n![Structure_main](Structure_main.png)\n","metadata":{},"id":"bdd098e2-49c6-43df-9151-050694970b75","cell_type":"markdown"},{"source":"# Install Imports and API Keys\n\nWe need to make sure our environment has the following packages: \n\n- Install `langchain` (we force the 0.0.184 version as later versions generate problems within the DataCamp workspace)\n- Install `tiktoken`, `wikipedia`, `pypdf`, `faiss-cpu`, `pinecone-client`.","metadata":{},"id":"42239f47-628c-4f50-aa9d-ee482177dd0c","cell_type":"markdown"},{"source":"!pip install openai==0.27.1\n!pip install langchain==0.0.184\n!pip install tiktoken\n!pip install wikipedia\n!pip install pypdf\n!pip install faiss-cpu\n!pip install pinecone-client","metadata":{"executionCancelledAt":null,"executionTime":36862,"lastExecutedAt":1709641454938,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install openai==0.27.1\n!pip install langchain==0.0.184\n!pip install tiktoken\n!pip install wikipedia\n!pip install pypdf\n!pip install faiss-cpu\n!pip install pinecone-client","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"7430d0f5-7341-464e-8d2e-c1f322edeb76","cell_type":"code","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: openai==0.27.1 in /home/repl/.local/lib/python3.8/site-packages (0.27.1)\nRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai==0.27.1) (2.31.0)\nRequirement already satisfied: tqdm in /home/repl/.local/lib/python3.8/site-packages (from openai==0.27.1) (4.66.2)\nRequirement already satisfied: aiohttp in /home/repl/.local/lib/python3.8/site-packages (from openai==0.27.1) (3.9.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai==0.27.1) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.20->openai==0.27.1) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests>=2.20->openai==0.27.1) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.20->openai==0.27.1) (2019.11.28)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (21.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (1.8.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai==0.27.1) (4.0.2)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: langchain==0.0.184 in /home/repl/.local/lib/python3.8/site-packages (0.0.184)\nRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.184) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.184) (1.4.40)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.184) (3.9.3)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.184) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.184) (0.5.14)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.184) (2.8.6)\nRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.184) (1.23.2)\nRequirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/repl/.local/lib/python3.8/site-packages (from langchain==0.0.184) (1.2.4)\nRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.184) (1.10.12)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.184) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from langchain==0.0.184) (8.2.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.184) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.184) (21.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.184) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.184) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.184) (1.8.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/repl/.local/lib/python3.8/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.184) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.184) (0.8.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<2,>=1->langchain==0.0.184) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2->langchain==0.0.184) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.0.184) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests<3,>=2->langchain==0.0.184) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain==0.0.184) (2019.11.28)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.184) (1.1.3)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.184) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.184) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.184) (3.0.9)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: tiktoken in /home/repl/.local/lib/python3.8/site-packages (0.6.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2022.8.17)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.8/dist-packages (from tiktoken) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2019.11.28)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: wikipedia in /home/repl/.local/lib/python3.8/site-packages (1.4.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (4.11.1)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wikipedia) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/repl/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.8/dist-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pypdf in /home/repl/.local/lib/python3.8/site-packages (4.1.0)\nRequirement already satisfied: typing_extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from pypdf) (4.9.0)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: faiss-cpu in /home/repl/.local/lib/python3.8/site-packages (1.8.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from faiss-cpu) (1.23.2)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pinecone-client in /home/repl/.local/lib/python3.8/site-packages (3.1.0)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/lib/python3/dist-packages (from pinecone-client) (2019.11.28)\nRequirement already satisfied: tqdm>=4.64.1 in /home/repl/.local/lib/python3.8/site-packages (from pinecone-client) (4.66.2)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.8/dist-packages (from pinecone-client) (4.9.0)\nRequirement already satisfied: urllib3>=1.26.0 in /home/repl/.local/lib/python3.8/site-packages (from pinecone-client) (2.2.1)\n"}]},{"source":"Before starting, make sure you have avaiable: \n- OpenAI API Key\n- Pinecone API Key and environment. \n\nTo get our API keys, we can set them in an .env document and load them into our environement using the 'load_dotenv()' command or define them directly. \n- To obtain OpenAI API Keys, you can follow the instructions [here](https://medium.com/forcodesake/a-step-by-step-guide-to-getting-your-api-key-2f6ee1d3e197). \n- To obtain Pinecone API keys, you can follow the instructions [here](https://medium.com/forcodesake/pinecone-api-chatgpt-artificial-intelligence-4332de128dd5). ","metadata":{},"cell_type":"markdown","id":"26db4425-1415-4422-878e-fc555b2fbe0b"},{"source":"# Basics\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom dotenv import load_dotenv\n\n# LangChain Training\n# LLM\nfrom langchain.llms import OpenAI\n\n# Document Loader\nfrom langchain.document_loaders import PyPDFLoader \n\n# Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# Embedding\nfrom langchain.embeddings import OpenAIEmbeddings \n\n# Vector DataBase\nfrom langchain.vectorstores import FAISS, Pinecone # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-based and permanent. \n\n# Chains\n#from langchain.chains.question_answering import load_qa_chain\n#from langchain.chains import ConversationalRetrievalChain","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1704815417233,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Basics\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom dotenv import load_dotenv\n\n# LangChain Training\n# LLM\nfrom langchain.llms import OpenAI\n\n# Document Loader\nfrom langchain.document_loaders import PyPDFLoader \n\n# Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n\n# Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# Embedding\nfrom langchain.embeddings import OpenAIEmbeddings \n\n# Vector DataBase\nfrom langchain.vectorstores import FAISS, Pinecone # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-based and permanent. \n\n# Chains\n#from langchain.chains.question_answering import load_qa_chain\n#from langchain.chains import ConversationalRetrievalChain"},"id":"0d8360cb-c7a0-4976-8fe9-5482d14b4495","cell_type":"code","execution_count":18,"outputs":[]},{"source":"import os\n\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\npinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\npinecone_env_key = os.environ[\"PINECONE_ENV_KEY\"]\n\n# Alternatively, you can set the API keys as follows:\n#os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n#PINECONE_API_KEY = \"34...\"\n#PINECONE_ENV_KEY = \"gcp-starter\"","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1704815417285,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\n\nopenai_api_key = os.environ[\"OPENAI_API_KEY\"]\npinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\npinecone_env_key = os.environ[\"PINECONE_ENV_KEY\"]\n\n# Alternatively, you can set the API keys as follows:\n#os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n#PINECONE_API_KEY = \"34...\"\n#PINECONE_ENV_KEY = \"gcp-starter\""},"id":"f0d70449-d284-43a6-ae31-df6e62ca1302","cell_type":"code","execution_count":19,"outputs":[]},{"source":"\n# PART 1: LANGCHAIN BASICS\n\n\nðŸŽ¯ **Objective:** Understand what is the LangChain library and all the elements that are required to generate a simple pipeline to query out documents. \n\n### **What is LangChain?**\n> LangChain is a framework for developing applications powered by language models.\n\nLangChain makes the hardest parts of working with AI models easier in two main ways:\n\n1. **Data-aware** - Bring external data, such as your files, other applications, and API data, to your LLMs\n2. **Agentic** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next. \n\n### **Why LangChain?**\n1. **Components** - Abstractions for working with language models, along with a collection of implementations for each abstraction. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not\n\n2. **Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together. A structured assembly of components for accomplishing specific higher-level tasks.\n\n3. **Speed ðŸš¢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n\n4. **Community ðŸ‘¥** - Wonderful discord and community support, meet ups, hackathons, etc.\n\nThough the usage of LLMs can be straightforward (text-in, text-out), when trying to build complex applications you'll quickly notice friction points. \n\n> LangChain helps with once you develop more complicated application and manage LLMs the way we want. ","metadata":{},"id":"8a853ff5-0446-4f29-8e78-66fbe290c7f9","cell_type":"markdown"},{"source":"## LangChain Components\n\nThe LangChain library contains multiple elements to ease the process of building complex applications using LangChain.\nIn this module we will focus mainly in 10 elements:\n\n**To load and process our documents**\n- Document Loaders\n- Text Splitters\n- Chat Messages *(Optional)*\n\n\n**To talk with our documents using NLP**\n- LLM model (GPT, Llama...)\n- Chains\n- Natural Language Retrieval\n- Metadata and Indexes\n- Memory *(Optional)*\n\n**Both Processes**\n- Text Embedding (OpenAI or Open-source models)\n- Vector Stores \n\n![Structure_basics](Structure_basics.png)\n","metadata":{},"id":"1f548002-dda8-4d89-9c1e-7057392c11c5","cell_type":"markdown"},{"source":"###  **The Model - Large Language Model of our choice**\nAn AI-powered LLM that takes text in and responses text out. \nThe default model is always ada-001, but we can explicitly choose the model of our preference. \n\nYou can check the list of all avaialble models [here](https://platform.openai.com/docs/models)","metadata":{},"cell_type":"markdown","id":"46cb81ae-fe1d-4108-a050-2e66c1bbf296"},{"source":"from langchain.llms import OpenAI\n\nchatgpt = OpenAI(\n                 model_name = \"gpt-3.5-turbo\", \n                 temperature= 0\n)\n\nprompt=\"Please, tell me some funny jokes\"\n\nprint(chatgpt(prompt))","metadata":{"executionCancelledAt":null,"executionTime":4491,"lastExecutedAt":1704815421777,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.llms import OpenAI\n\nchatgpt = OpenAI(\n                 model_name = \"gpt-3.5-turbo\", \n                 temperature= 0\n)\n\nprompt=\"Please, tell me some funny jokes\"\n\nprint(chatgpt(prompt))","outputsMetadata":{"0":{"height":477,"type":"stream"},"1":{"height":437,"type":"stream"}}},"id":"f1c96d08-9a35-40f5-a248-f0cb74a842fc","cell_type":"code","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"Sure, here are a few jokes for you:\n\n1. Why don't scientists trust atoms? Because they make up everything!\n\n2. Why don't skeletons fight each other? They don't have the guts!\n\n3. What do you call a bear with no teeth? A gummy bear!\n\n4. Why don't eggs tell jokes? Because they might crack up!\n\n5. Why did the scarecrow win an award? Because he was outstanding in his field!\n\n6. What do you call a fish wearing a crown? King Neptune!\n\n7. Why did the bicycle fall over? Because it was two-tired!\n\n8. How do you organize a space party? You \"planet\"!\n\n9. What do you call a snowman with a six-pack? An abdominal snowman!\n\n10. Why don't scientists trust atoms? Because they make up everything!\n\nRemember, humor is subjective, so what may be funny to one person may not be to another. Enjoy!\n"}]},{"source":"### **Chat Messages**\nLangChain allows us to segmentate prompts into three main types.(System, Human, AI)\n\n* **System** - Helpful background context that tell the AI its high-level behavior.\n* **Human** - Messages that represent the user input. \n* **AI** - Messages that show the response of the AI model, they work as examples to the model. \n\n\nFor more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)","metadata":{},"id":"e039302b-02bb-499c-8637-f4be1bd2e34c","cell_type":"markdown"},{"source":"from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nhigh_level_behavior = \"\"\"\n                       You are an AI bot that help people decide where to travel. \n                       Always recommend three destination with a short sentence for each.\n                      \"\"\"\n\nresponse = chatgpt(\n    [\n        SystemMessage(content=high_level_behavior),\n        AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n        HumanMessage(content=\"Where should I travel next?\"),\n    ]\n)\n\nprint(response.content)","metadata":{"executionCancelledAt":null,"executionTime":1197,"lastExecutedAt":1704815422974,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage, AIMessage\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nhigh_level_behavior = \"\"\"\n                       You are an AI bot that help people decide where to travel. \n                       Always recommend three destination with a short sentence for each.\n                      \"\"\"\n\nresponse = chatgpt(\n    [\n        SystemMessage(content=high_level_behavior),\n        AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n        HumanMessage(content=\"Where should I travel next?\"),\n    ]\n)\n\nprint(response.content)","outputsMetadata":{"0":{"height":57,"type":"stream"},"1":{"height":57,"type":"stream"}}},"id":"5a6c86bf-542a-43f3-a1e7-221ed9bf6918","cell_type":"code","execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":"That depends on your preferences! Could you please let me know what type of destination you are looking for? For example, do you prefer beach destinations, historical cities, or natural landscapes?\n"}]},{"source":"You can also pass more chat history with responses from the AI","metadata":{},"id":"ab221be1-08fd-4942-b06e-5945cce7b148","cell_type":"markdown"},{"source":"response = chatgpt(\n        [\n            SystemMessage(content=high_level_behavior),\n            AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n            HumanMessage(content=\"Where should I travel next?\"),\n            SystemMessage(content=\"What do you enjoy doing?\"),\n            HumanMessage(content=\"I love going to Museums?\"),\n        ]\n    )\n\nprint(response.content)","metadata":{"executionCancelledAt":null,"executionTime":3191,"lastExecutedAt":1704815426166,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"response = chatgpt(\n        [\n            SystemMessage(content=high_level_behavior),\n            AIMessage(content=\"Hello! I am a traveller assistant, how can I help you?\"),\n            HumanMessage(content=\"Where should I travel next?\"),\n            SystemMessage(content=\"What do you enjoy doing?\"),\n            HumanMessage(content=\"I love going to Museums?\"),\n        ]\n    )\n\nprint(response.content)","outputsMetadata":{"0":{"height":217,"type":"stream"},"1":{"height":217,"type":"stream"}}},"id":"035d9b68-ff7c-48fe-98e4-76961df7c480","cell_type":"code","execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"Based on your love for museums, here are three destinations you might enjoy:\n\n1. Paris, France: Known as the \"City of Museums,\" Paris is home to world-renowned museums like the Louvre, MusÃ©e d'Orsay, and Centre Pompidou, offering a rich collection of art and history.\n\n2. Florence, Italy: Florence is a treasure trove of art and culture, with museums like the Uffizi Gallery and Accademia Gallery housing masterpieces by Michelangelo, Botticelli, and more.\n\n3. New York City, USA: The Metropolitan Museum of Art, Museum of Modern Art (MoMA), and Guggenheim Museum are just a few of the many museums that make New York City a haven for art enthusiasts.\n"}]},{"source":"### **Text Embedding Model**\n\nWhen documents or string-variables are too long, things got quite complicated. \n\n**In order to be able to process them, we can embed and convert string variables into vectors** (a series of numbers that hold the semantic 'meaning' of your text).\n\n*What are embeddings?*\n\nTo put it simple, a number representation of your text. This list of numbers contains the information about the meaning of the text, so we can find similar text with similar meaning by seeing their number representation. \n\nMainly used when comparing different pieces of text or when dealing with huge texts. ","metadata":{},"id":"cf8f2cdd-0466-423f-8984-45a5e54585af","cell_type":"markdown"},{"source":"**TASK:**\n- First import the `Embeddings` model from langcgain.embeddings.\n- Define a text to embed. \n- Embed the text with the `.embed_query` command. ","metadata":{},"cell_type":"markdown","id":"fd391e69-5df4-4de4-8625-c590fb372227"},{"source":"# 1. Import the embedding model\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# 2. Create an instance of the model\nembeddings = OpenAIEmbeddings()\n\n# 3. Define a text to embed\ntext = \"Hi! It's time to go to a Museum!\"\n\n# 4. Embed the text\ntext_embedding = embeddings.embed_query(text)\nprint (f\"Your embedding is length {len(text_embedding)}\")\nprint (f\"Here's a sample: {text_embedding[:5]}...\")","metadata":{"executionCancelledAt":null,"executionTime":166,"lastExecutedAt":1704815426332,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the embedding model\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# 2. Create an instance of the model\nembeddings = OpenAIEmbeddings()\n\n# 3. Define a text to embed\ntext = \"Hi! It's time to go to a Museum!\"\n\n# 4. Embed the text\ntext_embedding = embeddings.embed_query(text)\nprint (f\"Your embedding is length {len(text_embedding)}\")\nprint (f\"Here's a sample: {text_embedding[:5]}...\")","outputsMetadata":{"0":{"height":77,"type":"stream"},"1":{"height":77,"type":"stream"}}},"id":"aeb78181-5f69-4672-a3f6-dd21f3068760","cell_type":"code","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":"Your embedding is length 1536\nHere's a sample: [-0.0022676142398267984, -0.003988472744822502, -0.0009102061158046126, -0.017572034150362015, -0.02116229198873043]...\n"}]},{"source":"### **Chains**\n\nConversation chains in the context of Langchain are a concept involving the sequential linking of multiple conversational elements to build complex interactions. The idea is to streamline and enhance the conversation flow.\n\nThe most basic chain is the `ConversationChain`. However, we will use the `load_qa_chain` to query questions about our documents, as its main function is optimized for this task. \n\nYou can go check all available chains in the [LangChain Documentation.](https://python.langchain.com/docs/modules/chains/)","metadata":{},"id":"d75d5161-1b4c-48f6-b0b0-9604585178b7","cell_type":"markdown"},{"source":"from langchain.chains import ConversationChain\n\nconversation = ConversationChain(llm=chatgpt)\nconversation.run(\"Hello!\")\n\n#from langchain.chains.question_answering import load_qa_chain \n\n#chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n#chain.run(input_documents = matches, question = enriched_query)","metadata":{"executionCancelledAt":null,"executionTime":628,"lastExecutedAt":1704815426960,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains import ConversationChain\n\nconversation = ConversationChain(llm=chatgpt)\nconversation.run(\"Hello!\")\n\n#from langchain.chains.question_answering import load_qa_chain \n\n#chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n#chain.run(input_documents = matches, question = enriched_query)"},"id":"569cf2d8-d2df-4c63-8bd6-a37745686a13","cell_type":"code","execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'Hello! How can I assist you today?'"},"metadata":{},"execution_count":24}]},{"source":"### Memory\nWhen interacting with a model, it is important to keep track of all interactions performed with it. \n\nTo overcome these limitations, langchain implements different types of memories to use in your application.\n\nIt is important to consider that storing all the interactions with the model can quickly escalate to a considerable amount of tokens to process every time we prompt the model. It is essential to bear in mind that ChatGPT has a token limit per interaction.\n\nYou can learn more about memory [here]([https://towardsdatascience.com/custom-memory-for-chatgpt-api-artificial-intelligence-python-722d627d4d6d])\n\n","metadata":{},"id":"35ae7808-4897-4435-aa62-b29625f0c5aa","cell_type":"markdown"},{"source":"from langchain.memory import ConversationSummaryBufferMemory\n\nmemory = ConversationSummaryBufferMemory(llm=chatgpt, max_token_limit=100)\n\nmemory.save_context({\"input\":  \"Can you recommend me where should I travel next?\"}, \n                    {\"output\": \"Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?\"})\n\nmemory.save_context({\"input\":  \"I love going to Museums\"}, \n                    {\"output\": \"Great then you should go to a cultural capital.\"})\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nconversation = ConversationChain(\n    llm=chatgpt, \n    memory = memory,\n    verbose=True\n)\n\nconversation.run(\"What cities do you recommend me?\")","metadata":{"executionCancelledAt":null,"executionTime":5737,"lastExecutedAt":1704815432697,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.memory import ConversationSummaryBufferMemory\n\nmemory = ConversationSummaryBufferMemory(llm=chatgpt, max_token_limit=100)\n\nmemory.save_context({\"input\":  \"Can you recommend me where should I travel next?\"}, \n                    {\"output\": \"Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?\"})\n\nmemory.save_context({\"input\":  \"I love going to Museums\"}, \n                    {\"output\": \"Great then you should go to a cultural capital.\"})\n\nchatgpt = ChatOpenAI(model_name = \"gpt-3.5-turbo\",\n                  temperature=0\n                 )\n\nconversation = ConversationChain(\n    llm=chatgpt, \n    memory = memory,\n    verbose=True\n)\n\nconversation.run(\"What cities do you recommend me?\")","outputsMetadata":{"0":{"height":337,"type":"stream"},"1":{"height":337,"type":"stream"},"2":{"height":97,"type":"stream"}}},"id":"e6dd11b7-b05e-435b-b8f2-27505a395c75","cell_type":"code","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\u001b[1m> Entering new ConversationChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Can you recommend me where should I travel next?\nAI: Hello! I am a traveller assistant, sure I can help you. What do you enjoy doing?\nHuman: I love going to Museums\nAI: Great then you should go to a cultural capital.\nHuman: What cities do you recommend me?\nAI:\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n"},{"output_type":"execute_result","data":{"text/plain":"\"There are many cities around the world that are known for their rich cultural heritage and numerous museums. Some popular options include Paris, France, which is home to the Louvre Museum and the MusÃ©e d'Orsay; London, England, where you can visit the British Museum and the Tate Modern; and New York City, USA, which boasts the Metropolitan Museum of Art and the Museum of Modern Art. These cities offer a wide range of museums and cultural experiences for you to explore.\""},"metadata":{},"execution_count":25}]},{"source":"### Dealing with Documents\n\nWe are here to deal with documents... so LangChain provides a wide variety of elements to deal with them. \n\nOne of the most important improvements of LangChain is that it allows us to upload documents and pass them to our model. \nWe consider a document as an object that holds a piece of text and metadata (more information about that text)\n\n- Document class\n- Document Loader\n- Document Retriever\n- Text Splitter\n- Index","metadata":{},"id":"e48c27ca-2d99-45d1-9d64-3436d095765e","cell_type":"markdown"},{"source":"**TASK**\n\n1. From langchain.schema import the `Document` class. \n2. Now define a document that has \n   - Text contained in page_content. \n   - Metada composed of document_id, document_source and document_create_time. ","metadata":{},"cell_type":"markdown","id":"bd360d81-3068-44b3-b72b-451623776022"},{"source":"# 1. Import the document class\nfrom langchain.schema import Document\n\n# 2. Define the document:\nDocument(page_content=\"Let's imagine this is a huge document with a lot of words and important stuff.\",\n         metadata={\n             'document_id' : 0000,\n             'document_source' : \"my_source\",\n             'document_create_time' : \"01/01/2000\"\n         })","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1704815432750,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the document class\nfrom langchain.schema import Document\n\n# 2. Define the document:\nDocument(page_content=\"Let's imagine this is a huge document with a lot of words and important stuff.\",\n         metadata={\n             'document_id' : 0000,\n             'document_source' : \"my_source\",\n             'document_create_time' : \"01/01/2000\"\n         })"},"cell_type":"code","id":"52d33238-ce16-45fd-b216-1b9166341193","execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":"Document(page_content=\"Let's imagine this is a huge document with a lot of words and important stuff.\", metadata={'document_id': 0, 'document_source': 'my_source', 'document_create_time': '01/01/2000'})"},"metadata":{},"execution_count":26}]},{"source":"#### Document Loaders\n\nDepending on where our data is stored, we will need a different type of loader:\n\n- The **Online Loader** is used for loading a document directly from the Internet. LangChain implements different types of loaders. For example, there is the `WikipediaLoader` that helps you loading Wikipedia pages or the `HNLoader` to take content directly from any HackerNews page.\n\n\n\n- The **Offline Loader** is used loading a document stored that are already installed in your machine. There are also different types of offline loaders such as the **HTML** loader for `.html` pages or the **PyPDFLoader** for `.pdf` documents.\n\nIn this tutorial, we will see an example of Online Loader by using the `WikipediaLoader` and the `HNLoader`, and an example of Offline Loader by using the PyPDFLoader.\n\nYou can find a list of the supported [LangChain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders) in the official documentation. Those Loaders are from external integrations, [native LangChain Loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/) can be found in the official documentation as well.","metadata":{},"id":"8c33e2ea-572e-4fbd-a348-5bcf0c961a8b","cell_type":"markdown"},{"source":"from langchain.document_loaders import WikipediaLoader\n \n# Load content from Wikipedia using WikipediaLoader\nloader = WikipediaLoader(\"Machine_learning\")\nwikipedia_data = loader.load() #It returns a list of documents\n\nwikipedia_data[0]","metadata":{"executionCancelledAt":null,"executionTime":5454,"lastExecutedAt":1704815438204,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.document_loaders import WikipediaLoader\n \n# Load content from Wikipedia using WikipediaLoader\nloader = WikipediaLoader(\"Machine_learning\")\nwikipedia_data = loader.load() #It returns a list of documents\n\nwikipedia_data[0]"},"id":"2599ff89-9838-4341-91a9-424eb3c4a49d","cell_type":"code","execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":"Document(page_content='Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field\\'s methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\\n\\n\\n== History and relationships to other fields ==\\n\\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb\\'s model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.By the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to re-evaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entitie', metadata={'title': 'Machine learning', 'summary': \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'})"},"metadata":{},"execution_count":27}]},{"source":"print(\"\\nPage Content: \", wikipedia_data[0].page_content)\nprint(\"\\nMeta Data: \", wikipedia_data[0].metadata)","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1704815438253,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(\"\\nPage Content: \", wikipedia_data[0].page_content)\nprint(\"\\nMeta Data: \", wikipedia_data[0].metadata)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"ae94cbf5-92bd-41a2-9f39-87c2bbb93803","execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":"\nPage Content:  Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\n\n\n== History and relationships to other fields ==\n\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.By the early 1960s an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to re-evaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entitie\n\nMeta Data:  {'title': 'Machine learning', 'summary': \"Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, generative artificial neural networks have been able to surpass many previous approaches in performance.Machine learning approaches have been applied to large language models, computer vision, speech recognition, email filtering, agriculture, and medicine, where it is too costly to develop algorithms to perform the needed tasks. ML is known in its application across business problems under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\\nThe mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis through unsupervised learning. From a theoretical point of view Probably approximately correct learning provides a framework for describing machine learning.\", 'source': 'https://en.wikipedia.org/wiki/Machine_learning'}\n"}]},{"source":"**TASK:**\n\nRepeat the previous procedure using `HNLoader` and `PyPDFLoader`. \n1. Import the corresponding Loader from langchain.document_loaders. \n2. Initialize the loader indicating the source of data. \n    - HNLoader -> https://news.ycombinator.com/item?id=34422627\n    - PDFLoader -> Docs/attentions.pdf\n    - PDFDirectoryLoader -> Docs/","metadata":{},"cell_type":"markdown","id":"a6c5bc9d-5c18-4bb6-b30b-025da04e0862"},{"source":"# Online Loader\nfrom langchain.document_loaders import HNLoader\nloader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\nhn_data = loader.load()\n\n# Load content from local PDFs\nfrom langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_data = loader.load()\n\n#We can use a directory loader to load more than one PDF at once. \n#loader = PyPDFDirectoryLoader(\"Docs/\")\n#pdf_directory_data = loader.load()","metadata":{"executionCancelledAt":null,"executionTime":6817,"lastExecutedAt":1704815445070,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Online Loader\nfrom langchain.document_loaders import HNLoader\nloader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")\nhn_data = loader.load()\n\n# Load content from local PDFs\nfrom langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_data = loader.load()\n\n#We can use a directory loader to load more than one PDF at once. \n#loader = PyPDFDirectoryLoader(\"Docs/\")\n#pdf_directory_data = loader.load()"},"cell_type":"code","id":"3a7b588e-8146-4df2-8e85-2e7d6d89cbb0","execution_count":29,"outputs":[]},{"source":"#### Text Splitter\n\n**Data Chunks and Model Tokenizer**\n\nTo efficiently handle data when building an LLM-based application, data needs to be divided in portions. Those are the so-called data chunks and the chunk size is highly determinant in the quality of the chatbot.\n\nThe tokenizer plays a crucial role in relation to data chunks when working with LLMs: \n- A **tokenizer is the tool used to convert text data into a format that can be processed by the model.**\n- Data is then stored in the vector stores in the tokenized format.\n\nTo convert the original data into tokens and split it in data chunks, we will use the **LangChain Text Splitter**.\n\nIf you are interested in more details about the tokenizer, the article [Unleashing the ChatGPT Tokenizer](https://medium.com/towards-data-science/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54) is for you!\n","metadata":{},"id":"da29979d-f779-41e1-a35c-598770a03139","cell_type":"markdown"},{"source":"By using Langchain, we can highly customize how to split our data:\n- **Split by chunks**: The most general approach is to split your data into chunks of a concrete size. In the following example, we will take the data that we have already loaded (`wikipedia_data`, `hn_data` and `pdf_data`) and we will split it in portions of 200 characters. \n\n_What will happen if the split based on character count breaks a word?_\n\nThere is the concept of \"chunk overlap\" that refers to a method where consecutive chunks of text share some common content. This technique is used to maintain context and coherence when a long document is divided into smaller parts due to the token limitations of LLMs. In this case, we will use a chunk size of 20 characters.\n\nSo let's split the Wikipedia data we have just loaded: ","metadata":{},"id":"c72a417e-7708-4d1e-bc99-0b0ba703dbe1","cell_type":"markdown"},{"source":"# Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# Advanced method - Split by chunks ________________________________________________________________________\n# Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\nprint(\"\\nSPLITTING BY CHUNKS\")\nwikipedia_chunks = text_splitter.split_documents(wikipedia_data)\nprint(\"Wikipedia Data - Now you have {0} number of chunks.\".format(len(wikipedia_chunks)))","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":77,"type":"stream"},"1":{"height":57,"type":"stream"},"2":{"height":77,"type":"stream"},"4":{"height":77,"type":"stream"},"5":{"height":57,"type":"stream"},"6":{"height":77,"type":"stream"}}},"id":"4df0ebc1-87a3-4755-a05d-4088db03b29d","cell_type":"code","execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":"\nSPLITTING BY CHUNKS\nWikipedia Data - Now you have 78 number of chunks.\n"}]},{"source":"**TASK:**\n\nGenerate the chunks for both `HNLoader` and `PyPDFLoader`. \n1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n3. Define a count_tokens function that will allow us to count the tokens of out text. \n4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n5. Apply the command `.split_documents`to our data. \n\nYou can define your own function for the HN data and use your the default function for the PDF Data.  ","metadata":{},"cell_type":"markdown","id":"2cb800f3-0b01-4bff-9291-1736cb5cdbe4"},{"source":"#_____________________________________________________________________PDF DATA\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\n# 5 - Apply the .split_document command\npdf_chunks = text_splitter.split_documents(pdf_data)\nprint(\"HN Data - Now you have {0} number of chunks.\".format(len(pdf_data)))\n\n\n#_____________________________________________________________________HN DATA\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# 3 - We use the default len, no need to do anything.\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20\n)\n\n\nhn_chunks = text_splitter.split_documents(hn_data)\nprint(\"Local PDF - Now you have {0} number of chunks.\".format(len(hn_chunks)))","metadata":{"executionCancelledAt":null,"executionTime":311,"lastExecutedAt":1704815445870,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#_____________________________________________________________________PDF DATA\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\n# 5 - Apply the .split_document command\npdf_chunks = text_splitter.split_documents(pdf_data)\nprint(\"HN Data - Now you have {0} number of chunks.\".format(len(pdf_data)))\n\n\n#_____________________________________________________________________HN DATA\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# 3 - We use the default len, no need to do anything.\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20\n)\n\n\nhn_chunks = text_splitter.split_documents(hn_data)\nprint(\"Local PDF - Now you have {0} number of chunks.\".format(len(hn_chunks)))","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"cell_type":"code","id":"bd61a742-dc5e-441a-a746-37d43012aceb","execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":"HN Data - Now you have 15 number of chunks.\nLocal PDF - Now you have 232 number of chunks.\n"}]},{"source":"We can make sure that the chunking has been successful by visualizing the distribution of chunk sizes. \nSince we have selected a chunk size of 200, the majority of our chunks should have this lenght:","metadata":{},"id":"ad447dd1-d5a5-4b2e-91ce-ef41539ddf73","cell_type":"markdown"},{"source":"# Quick data visualization to ensure chunking was successful\n\n# Create a list of token counts\ntoken_counts = [count_tokens(chunk.page_content) for chunk in pdf_chunks]\n\n# Create a DataFrame from the token counts\ndf = pd.DataFrame({'Token Count': token_counts})\n\n# Create a histogram of the token count distribution\ndf.hist(bins=40, )\n\n# Show the plot\nplt.show()","metadata":{"executionCancelledAt":null,"executionTime":324,"lastExecutedAt":1704815446194,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Quick data visualization to ensure chunking was successful\n\n# Create a list of token counts\ntoken_counts = [count_tokens(chunk.page_content) for chunk in pdf_chunks]\n\n# Create a DataFrame from the token counts\ndf = pd.DataFrame({'Token Count': token_counts})\n\n# Create a histogram of the token count distribution\ndf.hist(bins=40, )\n\n# Show the plot\nplt.show()"},"id":"6e41dd76-8f74-4372-808d-1c97099e671b","cell_type":"code","execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"source":"- **Split by pages**: If your data comes from documents organized in pages, there are methods that allow you to split data in pages to keep track of the page content. This method is specially useful when dealing with PDFs, as in the following example:","metadata":{},"id":"c149bde7-39f5-4912-83f2-fb238e4d72a7","cell_type":"markdown"},{"source":"# Simple method - Split by pages    ________________________________________________________________________\n# You need a PDF file in your environement. \nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_pages_chunks = loader.load_and_split()\npdf_pages_chunks\n\nprint(\"\\nSPLITTING BY PAGES\")\nprint(\"PDF Splited by Pages - You have {0} number of chunks.\".format(len(pdf_pages_chunks)))","metadata":{"executionCancelledAt":null,"executionTime":1292,"lastExecutedAt":1704815447487,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Simple method - Split by pages    ________________________________________________________________________\n# You need a PDF file in your environement. \nloader = PyPDFLoader(\"Docs/attentions.pdf\")\npdf_pages_chunks = loader.load_and_split()\npdf_pages_chunks\n\nprint(\"\\nSPLITTING BY PAGES\")\nprint(\"PDF Splited by Pages - You have {0} number of chunks.\".format(len(pdf_pages_chunks)))","outputsMetadata":{"0":{"height":77,"type":"stream"},"1":{"height":57,"type":"stream"},"2":{"height":137,"type":"stream"}}},"id":"87a8e84c-d658-4a7a-8144-d84fcb169928","cell_type":"code","execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":"\nSPLITTING BY PAGES\nPDF Splited by Pages - You have 16 number of chunks.\n"}]},{"source":"### Vector Stores\n\nVector stores, also known as vector databases, are specialized types of databases designed to efficiently handle and manipulate high-dimensional vector data. In our case, we will store the tokenized and splitted content, e.g., the data chunks in the format that LLMs can process.\n\nThere are different types of vector stores. Depending on the storage of the data, we can classify them as:\n- **Local Vector Stores**: This type of databases store the information in your local system. As an example of Local Vector Store, we will use FAISS.\n- **Online Vector Stores**: This type of databases store the information in the cloud. We will use Pinecone as out preferred option for Online Vector Stores.\n\nFAISS - EXAMPLE OF LOCAL VECTOR STORE","metadata":{},"id":"06933573-24eb-4ba2-bf05-4410912be27e","cell_type":"markdown"},{"source":"from langchain.vectorstores import FAISS  # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Get embedding model\nembeddings = OpenAIEmbeddings()\n\n# OPTION 1: FAISS (Facebook AI Similarity Search) Local _______________________________________________________________________________________\n# Create vector database\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n","metadata":{"executionCancelledAt":null,"executionTime":677,"lastExecutedAt":1704815448164,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.vectorstores import FAISS  # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\n# Get embedding model\nembeddings = OpenAIEmbeddings()\n\n# OPTION 1: FAISS (Facebook AI Similarity Search) Local _______________________________________________________________________________________\n# Create vector database\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n","outputsMetadata":{"0":{"height":580,"type":"stream"},"1":{"height":616,"type":"stream"}}},"id":"b7dc17d0-275f-4376-86e1-11661a2dd1ef","cell_type":"code","execution_count":34,"outputs":[]},{"source":"PINECONE - EXAMPLE OF ONLINE VECTOR STORE","metadata":{},"id":"1c4faca6-6ae9-453c-90d0-b31fc75a7872","cell_type":"markdown"},{"source":"import pinecone  #We need the Pinecone library to initialize our connection.\nfrom langchain.vectorstores import Pinecone # for the vector database part -- Pinecone is cloud-\n# OPTION 2: PINECONE Online\n     \n# We initialize pinecone\npinecone.init(      \n\tapi_key=os.getenv(\"PINECONE_API_KEY\"),      \n\tenvironment=os.getenv(\"PINECONE_ENV_KEY\")     \n) \n\n# Create a new pinecone index\n#pinecone.create(name=\"langchain\", dimension=1536, metric=\"cosine\")\n\n# We define the name of our index (in case the index is already created)\nindex_name = \"langchain\"\n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`. We use the Pinecone library of LangChain\ndb_Pinecone = Pinecone.from_documents(pdf_chunks, embeddings, index_name=index_name)","metadata":{"executionCancelledAt":null,"executionTime":3143,"lastExecutedAt":1704815451309,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pinecone  #We need the Pinecone library to initialize our connection.\nfrom langchain.vectorstores import Pinecone # for the vector database part -- Pinecone is cloud-\n# OPTION 2: PINECONE Online\n     \n# We initialize pinecone\npinecone.init(      \n\tapi_key=os.getenv(\"PINECONE_API_KEY\"),      \n\tenvironment=os.getenv(\"PINECONE_ENV_KEY\")     \n) \n\n# Create a new pinecone index\n#pinecone.create(name=\"langchain\", dimension=1536, metric=\"cosine\")\n\n# We define the name of our index (in case the index is already created)\nindex_name = \"langchain\"\n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`. We use the Pinecone library of LangChain\ndb_Pinecone = Pinecone.from_documents(pdf_chunks, embeddings, index_name=index_name)"},"id":"776ae3bd-a6d2-4066-881a-dd08cc9f9ede","cell_type":"code","execution_count":35,"outputs":[]},{"source":"### Natural Language Retrieval\nWe first start performing a semantic search within our Vector DataBase. ","metadata":{},"cell_type":"markdown","id":"e2e6151d-08d3-4c8f-8a7a-643c101bd701"},{"source":"# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndf = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=2)\nprint(matches)","metadata":{"executionCancelledAt":null,"executionTime":790,"lastExecutedAt":1704815452100,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndf = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=2)\nprint(matches)","outputsMetadata":{"0":{"height":417,"type":"stream"}}},"cell_type":"code","id":"826da9fb-03ba-4101-b793-3b9ac3203d79","execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":"[Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswaniâˆ—\\nGoogle Brain\\navaswani@google.comNoam Shazeerâˆ—\\nGoogle Brain\\nnoam@google.comNiki Parmarâˆ—\\nGoogle Research\\nnikip@google.comJakob Uszkoreitâˆ—\\nGoogle Research\\nusz@google.com\\nLlion Jonesâˆ—\\nGoogle Research\\nllion@google.comAidan N. Gomezâˆ— â€ \\nUniversity of Toronto\\naidan@cs.toronto.eduÅukasz Kaiserâˆ—\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhinâˆ— â€¡\\nillia.polosukhin@gmail.com', metadata={'source': 'Docs/attentions.pdf', 'page': 0, 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswaniâˆ—\\nGoogle Brain\\navaswani@google.comNoam Shazeerâˆ—\\nGoogle Brain\\nnoam@google.comNiki Parmarâˆ—\\nGoogle Research\\nnikip@google.comJakob Uszkoreitâˆ—\\nGoogle Research\\nusz@google.com\\nLlion Jonesâˆ—\\nGoogle Research\\nllion@google.comAidan N. Gomezâˆ— â€ \\nUniversity of Toronto\\naidan@cs.toronto.eduÅukasz Kaiserâˆ—\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhinâˆ— â€¡\\nillia.polosukhin@gmail.com'}), Document(page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word â€˜itsâ€™ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'Docs/attentions.pdf', 'page': 13, 'text': '.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word â€˜itsâ€™ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'})]\n"}]},{"source":"In the above section, we have seen how to retrieve the coincidences of you query in the documents in our vector store. Nevertheless, the output is a bit difficult to read. We can leverage the usage of LLMs by feeding the coincidences in our vector store to an LLM and let it generate a response in Natural Language using the additional information from our documents. We can do so by using the so-called **[LangChain Chains](https://python.langchain.com/docs/expression_language/get_started)**.","metadata":{},"id":"17489dca-2a53-40ab-9b12-13a65e1f8c53","cell_type":"markdown"},{"source":"# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndb_FAISS = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=4)\n\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\nchain.run(input_documents=matches, question = query)\n","metadata":{"executionCancelledAt":null,"executionTime":2516,"lastExecutedAt":1704815454616,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# LOCAL - FAISS\nfrom langchain.chains.question_answering import load_qa_chain \n\n# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\ndf = FAISS.from_documents(pdf_chunks, embeddings)\n\n# We can define how many similarities we want to get back by defining the variable k\nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\nmatches = db_FAISS.similarity_search(query, k=4)\n\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\nchain.run(input_documents=matches, question = query)\n","outputsMetadata":{"0":{"height":257,"type":"stream"}}},"id":"ccc9684f-f666-473f-8a48-9c8f53e413ab","cell_type":"code","execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'The authors of the article \"Attention Is All You Need\" are:\\n\\n1. Ashish Vaswani\\n2. Noam Shazeer\\n3. Niki Parmar\\n4. Jakob Uszkoreit\\n5. Llion Jones\\n6. Aidan N. Gomez\\n7. Åukasz Kaiser\\n8. Illia Polosukhin'"},"metadata":{},"execution_count":37}]},{"source":"**TASK:**\nRepeat the previous procedure using PINECONE.\n1. Define a query of your interest. \n2. Use the db_Pinecone database together with the `.similarity_search`command to perform a semantic search. \n3. Define a `load_qa_chain`and pass the matches together with the query to obtain a NLP based answer. ","metadata":{},"cell_type":"markdown","id":"812cbc3f-2d98-4085-8fa4-827cf7589e24"},{"source":"# ONLINE - PINECONE\n\n# 1. Define our query of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 2. Perform the semantic search in our vector database with the similarity_search command.  \nmatches = db_Pinecone.similarity_search(query, k=2)\n\n# 3. Define a load_qa_chain.\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Execute the chain with the prompt and the matches. \nchain.run(input_documents=matches, question = query)","metadata":{"executionCancelledAt":null,"executionTime":2211,"lastExecutedAt":1704815456827,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# ONLINE - PINECONE\n\n# 1. Define our query of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 2. Perform the semantic search in our vector database with the similarity_search command.  \nmatches = db_Pinecone.similarity_search(query, k=2)\n\n# 3. Define a load_qa_chain.\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Execute the chain with the prompt and the matches. \nchain.run(input_documents=matches, question = query)"},"id":"d164cac5-2244-4582-b567-e9ae4461afdd","cell_type":"code","execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":"'The authors of the article \"Attention Is All You Need\" are:\\n\\n1. Ashish Vaswani\\n2. Noam Shazeer\\n3. Niki Parmar\\n4. Jakob Uszkoreit\\n5. Llion Jones\\n6. Aidan N. Gomez\\n7. Åukasz Kaiser\\n8. Illia Polosukhin'"},"metadata":{},"execution_count":38}]},{"source":"### Indexes and Metadata\nWhen we upload data to our vector database, there is metadata that allows us to understand where the data is coming from. \nWhen dealing with PDFs, the source information allows us to know what pdf and page the info is coming from.","metadata":{},"id":"95ad7d06-178b-467b-979b-8976582012a7","cell_type":"markdown"},{"source":"#from langchain.embeddings import OpenAIEmbeddings\n#from langchain.indexes import index\n\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\n\nprint(\"______________________________________ THIRD MATCH\")\n\nprint(\"We can get the chunk text content and get: \\n\", matches[3].page_content)\nprint(\"\\nWe can get the chunk metadata and get: \\n\", matches[3].metadata)\nprint(\"\\nThe source of our match is: \\n\" , matches[3].metadata[\"source\"], \"and page\", matches[3].metadata[\"page\"])","metadata":{"executionCancelledAt":null,"executionTime":154,"lastExecutedAt":1704815456981,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#from langchain.embeddings import OpenAIEmbeddings\n#from langchain.indexes import index\n\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\n\nprint(\"______________________________________ THIRD MATCH\")\n\nprint(\"We can get the chunk text content and get: \\n\", matches[3].page_content)\nprint(\"\\nWe can get the chunk metadata and get: \\n\", matches[3].metadata)\nprint(\"\\nThe source of our match is: \\n\" , matches[3].metadata[\"source\"], \"and page\", matches[3].metadata[\"page\"])","outputsMetadata":{"0":{"height":517,"type":"stream"}}},"id":"97d1db0b-b343-4836-b43b-8e4ab735c033","cell_type":"code","execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":"______________________________________ THIRD MATCH\nWe can get the chunk text content and get: \n our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\nâˆ—Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n\nWe can get the chunk metadata and get: \n {'source': 'Docs/attentions.pdf', 'page': 0, 'text': 'our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\nâˆ—Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'}\n\nThe source of our match is: \n Docs/attentions.pdf and page 0\n"}]},{"source":"Now it is the time to put it all together and generate a simple pipeline to query our documents using a LLM model. ","metadata":{},"cell_type":"markdown","id":"f2e5f032-6e4d-40b2-b851-ccb9c86fc097"},{"source":"# PART 2: Loading and processing our documents\n\n\n\nPyPDFDirectoryLoader allows us to upload multiple PDFs at once. In our case, we have two PDFs in the Docs directory.","metadata":{},"id":"4d0282a5-73c9-44b8-87fd-eb175d676748","cell_type":"markdown"},{"source":"## **STEP 1 - LOADER**\n\nUse the `PDFDirectoryLoader` to upload all PDFs contained within the the Docs folder. ","metadata":{},"cell_type":"markdown","id":"89db047f-2475-4738-a726-458a5799a21e"},{"source":"#STEP 1 - LOADER\nfrom langchain.document_loaders import PyPDFDirectoryLoader\nloader = PyPDFDirectoryLoader(\"Docs/\")\n\ndata = loader.load()","metadata":{"executionCancelledAt":null,"executionTime":13250,"lastExecutedAt":1704815470231,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#STEP 1 - LOADER\nfrom langchain.document_loaders import PyPDFDirectoryLoader\nloader = PyPDFDirectoryLoader(\"Docs/\")\n\ndata = loader.load()"},"id":"a374a538-47c8-4679-a5d6-6c3e4a8a208c","cell_type":"code","execution_count":40,"outputs":[]},{"source":"## **STEP 2 - CHUNKING**\n\nGenerate the chunks for the PDFs contained in the directory. \n1. Import both RecursiveCharacterTextSplitter from langchain.text_splitter and GPT2TokenizerFast from transformers. \n2. Define a tokenizer with the following command: tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n3. Define a count_tokens function that will allow us to count the tokens of out text. \n4. Define the text_splitter with a chunk_size of 200, a chunk_overlap of 20 and the length_function we have just defined. \n5. Apply the command `.split_documents`to our data. ","metadata":{},"cell_type":"markdown","id":"d8aebdc5-1686-4401-89d2-3b59546f8d3c"},{"source":"#STEP 2 - CHUNKING OUR DATA\n#_____________________________________________________________________PDFs Data\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\n# 5 - Apply the .split_document command\nchunks = text_splitter.split_documents(data)\nprint(\"Multiple PDFs - Now you have {0} number of chunks.\".format(len(chunks)))","metadata":{"executionCancelledAt":null,"executionTime":1435,"lastExecutedAt":1704815471667,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#STEP 2 - CHUNKING OUR DATA\n#_____________________________________________________________________PDFs Data\n# 1 - Splitter\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter \n# 2 - Tokenizer\nfrom transformers import GPT2TokenizerFast  \n\n# 3 - Create function to count tokens\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ndef count_tokens(text: str) -> int:\n    return len(tokenizer.encode(text))\n\n# 4 - Define the splitter\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size      = 200, \n    chunk_overlap   = 20,\n    length_function = count_tokens # It uses len() by default. \n)\n\n# 5 - Apply the .split_document command\nchunks = text_splitter.split_documents(data)\nprint(\"Multiple PDFs - Now you have {0} number of chunks.\".format(len(chunks)))","outputsMetadata":{"0":{"height":57,"type":"stream"},"1":{"height":37,"type":"stream"}}},"cell_type":"code","id":"198312a1-bd33-4922-b5fd-1e402cd10c91","execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n"},{"output_type":"stream","name":"stdout","text":"Multiple PDFs - Now you have 610 number of chunks.\n"}]},{"source":"## **STEP 3 - EMBEDD AND UPLOAD THE DATA INTO A VECTORSTORE**\n\n**TASK**\n- Upload the data into the FAISS vector store using the `from_documents`command. ","metadata":{},"cell_type":"markdown","id":"8f548885-aee9-4b4a-bf1d-e996ed02c74d"},{"source":"# STEP 3 - EMBEDDING AND UPLOAD DATA INTO OUR VECTORSTORE\n\n# ___________________________________________________________________________ LOCAL VERSION\n\n# 1. Create vector database with FAISS\ndb_FAISS = FAISS.from_documents(chunks, embeddings)\n\n# Check similarity search is working\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\nprint(\"We found {0} number of similarities.\".format(len(matches)))\nfor match in matches:\n    print(\"\\n\", match.page_content)","metadata":{"executionCancelledAt":null,"executionTime":3149,"lastExecutedAt":1704815474816,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# STEP 3 - EMBEDDING AND UPLOAD DATA INTO OUR VECTORSTORE\n\n# ___________________________________________________________________________ LOCAL VERSION\n\n# 1. Create vector database with FAISS\ndb_FAISS = FAISS.from_documents(chunks, embeddings)\n\n# Check similarity search is working\nquery = \"Who created transformers?\"\nmatches = db_FAISS.similarity_search(query)\nprint(\"We found {0} number of similarities.\".format(len(matches)))\nfor match in matches:\n    print(\"\\n\", match.page_content)","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"04a6a94c-153d-438a-978b-df189a33fa6c","cell_type":"code","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":"We found 4 number of similarities.\n\n To the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n\n vision (Dosovitskiy et al., 2021), and many other areas (Oord et al., 2018; Jumper et al., 2021). Prior\nworks, such as CuBERT (Kanade et al., 2020), CodeBERT (Feng et al., 2020), PyMT5 (Clement et al.,\n2020), and CodeT5 (Wang et al., 2021), have applied transformers towards code understanding but\nthese mostly focus on code retrieval, classiï¬cation, and program repair. Several recent and concurrent\nefforts explore using large language models for program synthesis (Chen et al., 2021; Austin et al.,\n2021; Li et al., 2022; Fried et al., 2022) and its effectiveness (Vaithilingam et al., 2022). While they\nfocus on generating code in a single turn, we propose to factorize the speciï¬cations into multiple turns\n\n The Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n\n illia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n"}]},{"source":"# PART 3: Talking with our documents","metadata":{},"id":"37afb3bf-6c25-4562-b9c7-f32d17729cf9","cell_type":"markdown"},{"source":"## STEP 4 - DEFINE A CHAIN AND PERFORM THE SIMILARITY SEARCH\nGenerating a simple pipeline to query our documents with a load_qa_chain. \n**TASK**\n1. Import the `load_qa_chain`from the langchain.chains.question_answering library. \n2. Define a prompt of interest, like: \"Can you please tell me all the autors of the article Attention is all you need?\"\n3. Define the chain.\n4. Perform a semantic search with the `.similarity_search`. \n5. Execute the chain. ","metadata":{},"cell_type":"markdown","id":"b5d54ac1-c3ff-42c3-a9c8-151548914489"},{"source":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. Execute the chain to obtain a NLP based response. \nresponse = chain.run(input_documents = matches, question = query)\nprint(response)","metadata":{"executionCancelledAt":null,"executionTime":2286,"lastExecutedAt":1704815477102,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. Execute the chain to obtain a NLP based response. \nresponse = chain.run(input_documents = matches, question = query)\nprint(response)","outputsMetadata":{"0":{"height":217,"type":"stream"}}},"cell_type":"code","id":"85627ed4-89b0-475c-b7ea-caba04fce893","execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":"The authors of the article \"Attention Is All You Need\" are:\n\n1. Ashish Vaswani\n2. Noam Shazeer\n3. Niki Parmar\n4. Jakob Uszkoreit\n5. Llion Jones\n6. Aidan N. Gomez\n7. Åukasz Kaiser\n8. Illia Polosukhin\n"}]},{"source":"Now thta we already have a working pipeline to query our documents, we want to understand where our data is coming from. ","metadata":{},"cell_type":"markdown","id":"3570da7e-07ca-4dbd-a390-0f74aeabc044"},{"source":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)","metadata":{"executionCancelledAt":null,"executionTime":4579,"lastExecutedAt":1704815481681,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)","outputsMetadata":{"0":{"height":257,"type":"stream"}}},"id":"c9445c4e-6c17-4a2f-8a5e-0456873c8ac6","cell_type":"code","execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":"The authors of the article \"Attention Is All You Need\" are:\n\n1. Ashish Vaswani - Google Brain (source: attentions.pdf, page 0)\n2. Noam Shazeer - Google Brain (source: attentions.pdf, page 0)\n3. Niki Parmar - Google Research (source: attentions.pdf, page 0)\n4. Jakob Uszkoreit - Google Research (source: attentions.pdf, page 0)\n5. Llion Jones - Google Research (source: attentions.pdf, page 0)\n6. Aidan N. Gomez - University of Toronto (source: attentions.pdf, page 0)\n7. Åukasz Kaiser - Google Brain (source: attentions.pdf, page 0)\n8. Illia Polosukhin - (source: attentions.pdf, page 0)\n\nPlease note that the information provided is based on the given context.\n"}]},{"source":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches_and_scores = db_FAISS.similarity_search_with_score(query)\n\nmatches_and_scores","metadata":{"executionCancelledAt":null,"executionTime":172,"lastExecutedAt":1704815481854,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Can you please tell me all the autors of the article Attention is all you need?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches_and_scores = db_FAISS.similarity_search_with_score(query)\n\nmatches_and_scores"},"cell_type":"code","id":"e604eca4-f857-466f-b9f9-e491c3939533","execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":"[(Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswaniâˆ—\\nGoogle Brain\\navaswani@google.comNoam Shazeerâˆ—\\nGoogle Brain\\nnoam@google.comNiki Parmarâˆ—\\nGoogle Research\\nnikip@google.comJakob Uszkoreitâˆ—\\nGoogle Research\\nusz@google.com\\nLlion Jonesâˆ—\\nGoogle Research\\nllion@google.comAidan N. Gomezâˆ— â€ \\nUniversity of Toronto\\naidan@cs.toronto.eduÅukasz Kaiserâˆ—\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhinâˆ— â€¡\\nillia.polosukhin@gmail.com', metadata={'source': 'Docs/attentions.pdf', 'page': 0}),\n  0.37005126),\n (Document(page_content='formers . Oâ€™Reilly Media, Inc., 2022.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nÅukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS , 2017.\\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language\\nModel. https://github.com/kingoflolz/mesh-transformer-jax , May 2021.\\nYue Wang, Weishi Wang, Shaï¬q Joty, and Steven CH Hoi. CodeT5: Identiï¬er-aware uniï¬ed pre-\\ntrained encoder-decoder models for code understanding and generation. In EMNLP , 2021.', metadata={'source': 'Docs/incoder.pdf', 'page': 12}),\n  0.45794895),\n (Document(page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word â€˜itsâ€™ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'Docs/attentions.pdf', 'page': 13}),\n  0.4653595),\n (Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313â€“330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152â€“159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar TÃ¤ckstrÃ¶m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive', metadata={'source': 'Docs/attentions.pdf', 'page': 11}),\n  0.48549363)]"},"metadata":{},"execution_count":45}]},{"source":"Try to ask the model something that is completely out of scope, and see what happens!","metadata":{},"cell_type":"markdown","id":"09cfc58e-6995-4c1d-85b6-1cf185d480df"},{"source":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"What are the main problems to cook with olive oil?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)","metadata":{"executionCancelledAt":null,"executionTime":985,"lastExecutedAt":1704815482839,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"What are the main problems to cook with olive oil?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\n\n# 5. We define both the text and the metadata obtain from the semantic search.\ninput_text = [x.page_content for x in matches]\ninput_metadata= [x.metadata for x in matches]\n\n# 6. We define a metadata prompt with the metadata and ask the model to explicitily state the source. \nmeta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in the response\".format(input_metadata) \n\n# 7. We define an enriched query with the initial prompt and the metadata prompt. \nenriched_query = query + meta_data_enriching\n\n# 8. We execute the chain. \nresponse = chain.run(input_documents = matches, question = enriched_query)\nprint(response)","outputsMetadata":{"0":{"height":57,"type":"stream"}}},"cell_type":"code","id":"7812e8d8-e650-4444-b616-36834abe6c23","execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":"I'm sorry, but I don't have the information you're looking for. The provided information does not mention anything about problems related to cooking with olive oil.\n"}]},{"source":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Who went to mars?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\nmatches_and_scores = db_FAISS.similarity_search_with_score(query)\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":4841,"lastExecutedAt":1704815487680,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Import the load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain \n\n# 2. Define a prompt of interest. \nquery = \"Who went to mars?\"\n\n# 3. Define the chain\nchain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n\n# 4. Perform a similarity search. \nmatches = db_FAISS.similarity_search(query, k=1)\nmatches_and_scores = db_FAISS.similarity_search_with_score(query)\n\n\n"},"cell_type":"code","id":"06d68c3f-4306-49f4-8eca-df6bb17e7a50","execution_count":47,"outputs":[]},{"source":"matches_and_scores","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1704815487730,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"matches_and_scores"},"cell_type":"code","id":"e1201e60-296c-4a64-a9a9-2f1fa44a2fed","execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":"[(Document(page_content='arXiv preprint arXiv:2005.14050 , 2020.\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell,\\nA., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T.,\\nChild, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse,\\nC., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,\\nJ., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and', metadata={'source': 'Docs/codex.pdf', 'page': 14}),\n  0.52742505),\n (Document(page_content='Wallach, H., Fergus, R., Vishwanathan, S., and Garnett,\\nR. (eds.), Advances in Neural Information Processing\\nSystems , volume 30. Curran Associates, Inc., 2017. URL\\nhttps://proceedings .neurips .cc/paper/2017/\\nfile/3f5ee243547dee91fbd053c1c4a845aa-\\nPaper .pdf.', metadata={'source': 'Docs/codex.pdf', 'page': 17}),\n  0.5399054),\n (Document(page_content='the ACM , 1970.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. In NeurIPS , 2020.\\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell Stern, and Jakob Uszkoreit. KERMIT: Genera-\\ntive insertion-based modeling for sequences. arXiv preprint arXiv:1906.01604 , 2019.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large', metadata={'source': 'Docs/incoder.pdf', 'page': 10}),\n  0.5414845),\n (Document(page_content='Hiroaki Hayashi (hiroakihayashi@salesforce.com), Yingbo Zhou (yingbo.zhou@salesforce.com), Caiming\\nXiong (cxiong@salesforce.com).\\n1arXiv:2203.13474v5  [cs.LG]  27 Feb 2023', metadata={'source': 'Docs/codegen.pdf', 'page': 0}),\n  0.5431116)]"},"metadata":{},"execution_count":48}]},{"source":"print(matches[0].page_content)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1704815487782,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(matches[0].page_content)","outputsMetadata":{"0":{"height":157,"type":"stream"}}},"cell_type":"code","id":"17d65e83-4ba5-4c16-82d7-946d31de1e4d","execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":"arXiv preprint arXiv:2005.14050 , 2020.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell,\nA., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T.,\nChild, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse,\nC., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark,\nJ., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and\n"}]},{"source":"Try other queries and talk with your documents!","metadata":{},"cell_type":"markdown","id":"e9aeec8e-842f-4505-b895-8818cc7021fb"},{"source":"def asking_your_model(query, k):\n    # Define the chain\n    chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n    #Perform a similarity search. \n    matches = db_FAISS.similarity_search(query, k=k)\n    #We define both the text and the metadata obtain from the semantic search.\n    input_text = [x.page_content for x in matches]\n    input_metadata= [x.metadata for x in matches]\n    #We define a metadata prompt with the metadata and ask the model to explicitily state the source. \n    meta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in       the response\".format(input_metadata) \n    #We define an enriched query with the initial prompt and the metadata prompt. \n    enriched_query = query + meta_data_enriching\n    #We execute the chain. \n    response = chain.run(input_documents = matches, question = enriched_query)\n    return response\n    ","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1704815487837,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def asking_your_model(query, k):\n    # Define the chain\n    chain = load_qa_chain(chatgpt, chain_type=\"stuff\")\n    #Perform a similarity search. \n    matches = db_FAISS.similarity_search(query, k=k)\n    #We define both the text and the metadata obtain from the semantic search.\n    input_text = [x.page_content for x in matches]\n    input_metadata= [x.metadata for x in matches]\n    #We define a metadata prompt with the metadata and ask the model to explicitily state the source. \n    meta_data_enriching = \"The provided information has been extracted from {0}, please state info sources (both pdf and page) in       the response\".format(input_metadata) \n    #We define an enriched query with the initial prompt and the metadata prompt. \n    enriched_query = query + meta_data_enriching\n    #We execute the chain. \n    response = chain.run(input_documents = matches, question = enriched_query)\n    return response\n    "},"cell_type":"code","id":"9c4b26ec-dc92-49bc-9955-2516e8a0cca6","execution_count":50,"outputs":[]},{"source":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is functional correctness?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","metadata":{"executionCancelledAt":null,"executionTime":2288,"lastExecutedAt":1704815490125,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is functional correctness?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","outputsMetadata":{"0":{"height":77,"type":"stream"}}},"id":"64a2c499-6ef6-424d-b58d-fbd571dd0522","cell_type":"code","execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":"Functional correctness refers to the property of a program or code that it performs its intended function accurately and correctly. It means that the program produces the expected output or behavior for a given set of inputs or test cases. The information sources for this definition are 'Docs/codex.pdf' (page 1) and 'Docs/codex.pdf' (page 13).\n"}]},{"source":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is the multi-head attention in a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","metadata":{"executionCancelledAt":null,"executionTime":3219,"lastExecutedAt":1704815493344,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What is the multi-head attention in a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","outputsMetadata":{"0":{"height":197,"type":"stream"}}},"cell_type":"code","id":"e6eb01de-7644-45a0-a88c-f290ee351640","execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":"The multi-head attention in a transformer is a mechanism that allows the model to jointly attend to information from different representation subspaces at different positions. It is used in three different ways in the transformer model: in encoder-decoder attention layers, in self-attention layers in the encoder, and in self-attention layers in the decoder. \n\nThe multi-head attention is achieved by splitting the queries, keys, and values into multiple heads and applying attention independently to each head. The outputs of the attention heads are then concatenated and linearly transformed to obtain the final output. \n\nThis information is from the \"Attention Is All You Need\" paper, specifically from pages 4 and 1.\n"}]},{"source":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What are the main components of a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","metadata":{"executionCancelledAt":null,"executionTime":3031,"lastExecutedAt":1704815496375,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains.question_answering import load_qa_chain \n\n# Check similarity search is working\nquery = \"What are the main components of a transformer?\"\nresponse = asking_your_model(query, k=4)\nprint(response)","outputsMetadata":{"0":{"height":117,"type":"stream"}}},"cell_type":"code","id":"0925bf69-fc5b-47d8-8f7d-322dab181ae3","execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":"The main components of a Transformer are the encoder and decoder stacks. The encoder stack consists of N=6 identical layers, each containing two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder stack also has N=6 identical layers, with the addition of a third sub-layer that performs multi-head attention over the encoder's output. Both the encoder and decoder stacks use residual connections and layer normalization. This information is from the document \"Docs/attentions.pdf\", pages 1 and 2.\n"}]}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}