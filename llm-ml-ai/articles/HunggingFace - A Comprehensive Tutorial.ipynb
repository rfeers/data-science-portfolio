{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219bd115",
   "metadata": {},
   "source": [
    "<font size=\"6\">**Hugging Face** - A Comprehensive Guide</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c11e89",
   "metadata": {},
   "source": [
    "**Requirements**\n",
    "\n",
    "For this tutorial, the following libraries are needed: \n",
    "- Throughout the whole tutorial, we will be using the `transformers` library. \n",
    "- For the fine-tuning either `pytorch` or `tensorflow` are required. (This Notebook will be implemented with `pytorch`)\n",
    "- To push the fine-tuned model to HuggingFace, the `HuggingFace_hub`library is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6ceab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install transformers\n",
    "#%pip install torch\n",
    "#%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14db8b4",
   "metadata": {},
   "source": [
    "# Core Components of Hugging Face\n",
    "## Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c03e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'learning_&_educational', 'score': 0.46964123845100403}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Define the model_name extracted form the HuggingFace_Hub\n",
    "model_name = \"cardiffnlp/tweet-topic-21-multi\"\n",
    "\n",
    "# We call the model class\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# We call the tokenizer class\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model = model, tokenizer = tokenizer)\n",
    "\n",
    "# We can easily execute the model by submiting an input\n",
    "input_  = \"I've been waiting for this tutorial all my life!\"\n",
    "output_ = classifier(input_)\n",
    "print(output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76f5ac",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "A tokenizer basically puts a text in a mathematical representation that the model understands. We can invoke the tokenier and tokenize a text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f75d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized_sequence: \n",
      " {'input_ids': [0, 36949, 10, 40878, 13931, 16, 1341, 2007, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Tokens: \n",
      " ['Using', 'Ġa', 'Ġtransformer', 'Ġsequence', 'Ġis', 'Ġquite', 'Ġsimple']\n",
      "Tokens_ids: \n",
      " [36949, 10, 40878, 13931, 16, 1341, 2007]\n",
      "Using a transformer sequence is quite simple\n"
     ]
    }
   ],
   "source": [
    "# Define the model_name extracted form the HuggingFace_Hub\n",
    "model_name = \"cardiffnlp/tweet-topic-21-multi\"\n",
    "\n",
    "# We call the model class\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# We call the tokenizer class\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "sequence = \"Using a transformer sequence is quite simple\"\n",
    "tokenized_sequence = tokenizer(sequence)\n",
    "\n",
    "#We obtain a dictionary with the tokenizes ids and an attention_mask that emphasize what to pay attention and what not to. \n",
    "print(\"Tokenized_sequence: \\n\", tokenized_sequence)\n",
    "\n",
    "#We get the tokens (101 is beginning of sentence and 102 is ending of sentence )\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(\"Tokens: \\n\", tokens)\n",
    "\n",
    "#We get the corresponding ids for each token\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens_ids: \\n\", tokens_ids)\n",
    "\n",
    "#We get back the original word.\n",
    "decoded_string = tokenizer.decode(tokens_ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad467c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized_sequence: \n",
      " {'input_ids': [101, 32935, 169, 99662, 10165, 30265, 10124, 31324, 16205, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Tokens: \n",
      " ['Using', 'a', 'transform', '##er', 'sequence', 'is', 'quite', 'simple']\n",
      "Tokens_ids: \n",
      " [32935, 169, 99662, 10165, 30265, 10124, 31324, 16205]\n",
      "Using a transformer sequence is quite simple\n"
     ]
    }
   ],
   "source": [
    "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "# We call the model class\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "#We call the tokenizer class\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sequence = \"Using a transformer sequence is quite simple\"\n",
    "tokenized_sequence = tokenizer(sequence)\n",
    "\n",
    "#We obtain a dictionary with the tokenizes ids and an attention_mask that emphasize what to pay attention and what not to. \n",
    "print(\"Tokenized_sequence: \\n\", tokenized_sequence)\n",
    "\n",
    "#We get the tokens (101 is beginning of sentence and 102 is ending of sentence )\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(\"Tokens: \\n\", tokens)\n",
    "\n",
    "#We get the corresponding ids for each token\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Tokens_ids: \\n\", tokens_ids)\n",
    "\n",
    "#We get back the original word.\n",
    "decoded_string = tokenizer.decode(tokens_ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35521db",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2615f09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87594</th>\n",
       "      <td>5735d259012e2f140011a09d</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what US state did Kathmandu first establish...</td>\n",
       "      <td>{'text': ['Oregon'], 'answer_start': [229]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87595</th>\n",
       "      <td>5735d259012e2f140011a09e</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What was Yangon previously known as?</td>\n",
       "      <td>{'text': ['Rangoon'], 'answer_start': [414]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87596</th>\n",
       "      <td>5735d259012e2f140011a09f</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>With what Belorussian city does Kathmandu have...</td>\n",
       "      <td>{'text': ['Minsk'], 'answer_start': [476]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87597</th>\n",
       "      <td>5735d259012e2f140011a0a0</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>{'text': ['1975'], 'answer_start': [199]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87598</th>\n",
       "      <td>5735d259012e2f140011a0a1</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>What is KMC an initialism of?</td>\n",
       "      <td>{'text': ['Kathmandu Metropolitan City'], 'ans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87599 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id                     title  \\\n",
       "0      5733be284776f41900661182  University_of_Notre_Dame   \n",
       "1      5733be284776f4190066117f  University_of_Notre_Dame   \n",
       "2      5733be284776f41900661180  University_of_Notre_Dame   \n",
       "3      5733be284776f41900661181  University_of_Notre_Dame   \n",
       "4      5733be284776f4190066117e  University_of_Notre_Dame   \n",
       "...                         ...                       ...   \n",
       "87594  5735d259012e2f140011a09d                 Kathmandu   \n",
       "87595  5735d259012e2f140011a09e                 Kathmandu   \n",
       "87596  5735d259012e2f140011a09f                 Kathmandu   \n",
       "87597  5735d259012e2f140011a0a0                 Kathmandu   \n",
       "87598  5735d259012e2f140011a0a1                 Kathmandu   \n",
       "\n",
       "                                                 context  \\\n",
       "0      Architecturally, the school has a Catholic cha...   \n",
       "1      Architecturally, the school has a Catholic cha...   \n",
       "2      Architecturally, the school has a Catholic cha...   \n",
       "3      Architecturally, the school has a Catholic cha...   \n",
       "4      Architecturally, the school has a Catholic cha...   \n",
       "...                                                  ...   \n",
       "87594  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87595  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87596  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87597  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "87598  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "\n",
       "                                                question  \\\n",
       "0      To whom did the Virgin Mary allegedly appear i...   \n",
       "1      What is in front of the Notre Dame Main Building?   \n",
       "2      The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                      What is the Grotto at Notre Dame?   \n",
       "4      What sits on top of the Main Building at Notre...   \n",
       "...                                                  ...   \n",
       "87594  In what US state did Kathmandu first establish...   \n",
       "87595               What was Yangon previously known as?   \n",
       "87596  With what Belorussian city does Kathmandu have...   \n",
       "87597  In what year did Kathmandu create its initial ...   \n",
       "87598                      What is KMC an initialism of?   \n",
       "\n",
       "                                                 answers  \n",
       "0      {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1      {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2      {'text': ['the Main Building'], 'answer_start'...  \n",
       "3      {'text': ['a Marian place of prayer and reflec...  \n",
       "4      {'text': ['a golden statue of the Virgin Mary'...  \n",
       "...                                                  ...  \n",
       "87594        {'text': ['Oregon'], 'answer_start': [229]}  \n",
       "87595       {'text': ['Rangoon'], 'answer_start': [414]}  \n",
       "87596         {'text': ['Minsk'], 'answer_start': [476]}  \n",
       "87597          {'text': ['1975'], 'answer_start': [199]}  \n",
       "87598  {'text': ['Kathmandu Metropolitan City'], 'ans...  \n",
       "\n",
       "[87599 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load a dataset by name\n",
    "dataset = load_dataset('squad')\n",
    "\n",
    "# The dataset object is now a DatasetDict with predefined splits\n",
    "print(dataset)  # Access the first sample from the training set\n",
    "pd.DataFrame.from_dict(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08e7d2",
   "metadata": {},
   "source": [
    "# Getting Started with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb300ce",
   "metadata": {},
   "source": [
    "## Using Pre-trained Models\n",
    "Hugging Faces provides the most used NLP library on GitHub with over 115k stars. It offers a wide variety of models with different tasks to perform. You can go check all possible tasks in [here](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.model). \n",
    "\n",
    "The most common ones are `text classification`, `QA`, `translation`... among others. \n",
    "\n",
    "Our first example will be using a **sentiment analysis** (text classification) model to infere the sentiment of an input text. \n",
    "\n",
    "The `pipeline()` command is a high-level API that allow users to easily apply complex models to real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8566f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.56911301612854}]\n"
     ]
    }
   ],
   "source": [
    "# Define the model_name extracted form the HuggingFace_Hub\n",
    "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
    "\n",
    "# We call the model class\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# We call the tokenizer class\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# We define a pipeline object with the task to be performed, the selected model and the select tokenizer. \n",
    "# If we initialize the pipeline class with only the task names, it will be populated with default model and tokenizer. \n",
    "classifier = pipeline(\"sentiment-analysis\", model = model, tokenizer = tokenizer)\n",
    "#classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# We can easily execute the model by submiting an input\n",
    "input_  = \"I've been waiting for this tutorial all my life!\"\n",
    "output_ = classifier(input_)\n",
    "print(output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a158ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.56911301612854}, {'label': 'negative', 'score': 0.9502121806144714}]\n"
     ]
    }
   ],
   "source": [
    "# We can easily execute the model by submiting an input\n",
    "input_ = [\"I've been waiting for this tutorial all my life!\",\"I hate this...\" ]\n",
    "output_ = classifier(input_)\n",
    "print(output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a192e198",
   "metadata": {},
   "source": [
    "## Fine-tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45abc162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "#____________________________________________ 1. prepare dataset\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Loading the dataset to train our model\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d5fc9",
   "metadata": {},
   "source": [
    "Checking the dataset to be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02980e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  label  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going      1   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "2      088c60f138                          my boss is bullying me...      0   \n",
       "3      9642c003ef                     what interview! leave me alone      0   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "...           ...                                                ...    ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...      0   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...      0   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...      2   \n",
       "27479  ed167662a5                         But it was worth it  ****.      2   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...      1   \n",
       "\n",
       "      label_text  \n",
       "0        neutral  \n",
       "1       negative  \n",
       "2       negative  \n",
       "3       negative  \n",
       "4       negative  \n",
       "...          ...  \n",
       "27476   negative  \n",
       "27477   negative  \n",
       "27478   positive  \n",
       "27479   positive  \n",
       "27480    neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "155bf4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.794825</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.625867</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.597192</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.597192406654358,\n",
       " 'eval_accuracy': 0.2,\n",
       " 'eval_runtime': 0.3246,\n",
       " 'eval_samples_per_second': 30.809,\n",
       " 'eval_steps_per_second': 6.162,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#____________________________________________ 2. load pretrained Tokenizer, call it with a dataset -> encoding.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "#____________________________________________  3. Build a PyTorch Dataset with encodings.\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(10))\n",
    "\n",
    "#____________________________________________  4. Train the model\n",
    "training_args = TrainingArguments(output_dir=\"trainer_output\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#____________________________________________  5. Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ef22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"Fine_Tuned_Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcdf1e",
   "metadata": {},
   "source": [
    "## Sharing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ade15",
   "metadata": {},
   "source": [
    "HuggingFace is a community-driven platform. This means we all can share out models or fine-tuned versions with the whole community. \n",
    "\n",
    "To do so, we first need to log in the HuggingFace account. The HuggingFace_hub library contains a specific module to allows us to use it in Jupyter Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f3c3d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the actual path to your fine-tuned model\n",
    "model_path = \"Fine_Tuned_Models\"  \n",
    "\n",
    "# We get our model\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28723414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6c8f9ff1e34b7ba78e0b75cfac0a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a50f354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a186da9b1b04565980a9d3799542e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rfeers/distilbert-base-multilingual-cased-sentiments-student-fine-tuned-data-camp/commit/6e00368ebca1c2b68fd32b387338c948ea71aed8', commit_message='Upload DistilBertForSequenceClassification', commit_description='', oid='6e00368ebca1c2b68fd32b387338c948ea71aed8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model.push_to_hub(\"distilbert-base-multilingual-cased-sentiments-student-fine-tuned-data-camp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cf24d",
   "metadata": {},
   "source": [
    "# Use Cases and Applications\n",
    "\n",
    "If we want to standardize any NLP process, with hugging face it usually involves of three simple steps that take less than 5 lines of code: \n",
    "1. Defining a model object with the pipeline class (and the corresponding model and tokenizer). \n",
    "2. Define the input text or prompt.\n",
    "3. Execute the pre-trained model with our input and observe the output. \n",
    "\n",
    "## Text Classification\n",
    "Text classification is a fundamental task in natural language processing (NLP) where a piece of text is assigned to one or more categories. This can be used for a variety of applications such as spam detection, sentiment analysis, topic labeling, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9144f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9909080266952515}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained text classification model.\n",
    "classifier = pipeline(\"text-classification\",model='lxyuan/distilbert-base-multilingual-cased-sentiments-student')\n",
    "\n",
    "# Input to be classified\n",
    "input_ = \"I absolutely love the transformers library!\"\n",
    "\n",
    "# Perform classification\n",
    "output_ = classifier(input_)\n",
    "\n",
    "# Observer the result\n",
    "print(output_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b11ac",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "The process of generating text is a fascinating aspect of NLP where a model produces human-like text. \n",
    "\n",
    "It has a wide range of applications from creating chatbot responses to generating creative writing. \n",
    "\n",
    "**The core idea is to train a model on a large corpus of text**, enabling it to learn patterns, styles, and structures of language. \n",
    "As you can imagine, the most expensive part is precisely the training of the model. \n",
    "\n",
    "So let’s see how we can apply a Text Generation task with less than 5 lines of code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3c3dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world dominated by AI, the problem is often ignored, but these efforts prove remarkably effective. In a study published in February 2014 in the journal Nature Computational Biology, researchers at Harvard Medical School identified the next step to build a software system so\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "prompt = \"In a world dominated by AI,\"\n",
    "\n",
    "generated_text = generator(prompt, max_length=50)[0]['generated_text']\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01f8bf",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "Question Answering, or commonly referred as QA, is a field in NLP focused on building systems that automatically answer questions posed by humans in natural language. \n",
    "\n",
    "QA systems are widely used in various applications, such as virtual assistants, customer support, and information retrieval systems.\n",
    "\n",
    "QA systems can be broadly categorized into two types:\n",
    "- **Open-domain QA:** Answers questions based on a broad range of knowledge, often sourced from the internet or large databases.\n",
    "- **Closed-domain QA:** Focuses on a specific domain, like medicine or law, and answers questions from a limited dataset.\n",
    "\n",
    "These systems typically use a combination of natural language understanding to interpret the question and information retrieval to find relevant answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd5e3953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9549079537391663, 'start': 121, 'end': 130, 'answer': '2,140,526'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline('question-answering', model='distilbert-base-uncased-distilled-squad')\n",
    "\n",
    "context = \"\"\"Paris is the capital and most populous city of France. The city has an area of 105 square kilometers and a population of 2,140,526 residents.\"\"\"\n",
    "question = \"What is the population of Paris?\"\n",
    "\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36acf7",
   "metadata": {},
   "source": [
    "## Translation\n",
    "The final use-case is translation. Machine Translation  is a subfield of computational linguistics that focuses on translating text or speech from one language to another using software. With the advent of deep learning, machine translation has made significant strides, particularly with models like Neural Machine Translation (NMT) that use large neural networks.\n",
    "\n",
    "Modern NMT systems, unlike traditional rule-based or statistical translation models, learn to translate by training on large datasets of bilingual text. They use sequence-to-sequence architectures, where one part of the network encodes the source text and another decodes it into the target language, often with impressive fluency and accuracy.\n",
    "\n",
    "A simple example using hugging face would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44276149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/josepferrersanchez/.pyenv/versions/3.10.11/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dies ist ein großer Tag für die Wissenschaft!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the translation pipeline for English to Spanish\n",
    "translator = pipeline('translation_en_to_de')\n",
    "\n",
    "# Text to translate from English to Spanish\n",
    "text_to_translate = \"This is a great day for science!\"\n",
    "\n",
    "# Perform the translation\n",
    "translation = translator(text_to_translate, max_length=40)\n",
    "\n",
    "# Print the translated text\n",
    "print(translation[0]['translation_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HuggingFace_env",
   "language": "python",
   "name": "huggingface_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
