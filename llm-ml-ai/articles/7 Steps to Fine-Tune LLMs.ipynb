{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219bd115",
   "metadata": {},
   "source": [
    "<font size=\"6\">**7 steps to fine-tune a LLM**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c632b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2514d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install accelerate -U\n",
    "# %pip install pandas, numpy, huggingface\n",
    "# %pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e957f09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2c11e89",
   "metadata": {},
   "source": [
    "**Requirements**\n",
    "\n",
    "For this tutorial, the following libraries are needed: \n",
    "- Throughout the whole tutorial, we will be using the `transformers` library. \n",
    "- For the fine-tuning either `pytorch` or `tensorflow` are required. (This Notebook will be implemented with `pytorch`)\n",
    "- To push the fine-tuned model to HuggingFace, the `HuggingFace_hub`library is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de6ceab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install torch\n",
    "# %pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14db8b4",
   "metadata": {},
   "source": [
    "## STEP 1 - Having our concrete objective clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e084789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fce278f7",
   "metadata": {},
   "source": [
    "## STEP 2 - Choose a pre-trained model and a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799b779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7774098",
   "metadata": {},
   "source": [
    "## STEP 3 - Load the data to use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6105658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "df = pd.DataFrame(dataset['train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c23e4f",
   "metadata": {},
   "source": [
    "## STEP 4 - Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "707d5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Loading the dataset to train our model\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ceff71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c407b4",
   "metadata": {},
   "source": [
    "## STEP 5 - Initialize our base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea969e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24fcc88",
   "metadata": {},
   "source": [
    "## STEP 6 - Evaluate method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5f739ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa1718",
   "metadata": {},
   "source": [
    "## STEP 7 - Fine-tune using the Trainer Method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4eebb612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josepferrersanchez/projects/large-language-models/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 75/75 [02:38<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 158.3406, 'train_samples_per_second': 1.895, 'train_steps_per_second': 0.474, 'train_loss': 1.1476851399739583, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=1.1476851399739583, metrics={'train_runtime': 158.3406, 'train_samples_per_second': 1.895, 'train_steps_per_second': 0.474, 'train_loss': 1.1476851399739583, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"test_trainer\",\n",
    "   #evaluation_strategy=\"epoch\",\n",
    "   per_device_train_batch_size=1,  # Reduce batch size here\n",
    "   per_device_eval_batch_size=1,    # Optionally, reduce for evaluation as well\n",
    "   gradient_accumulation_steps=4\n",
    "   )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=small_train_dataset,\n",
    "   eval_dataset=small_eval_dataset,\n",
    "   compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c3c6a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.13it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "trainer.save_model(\"Fine_Tuned_Models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e23c2",
   "metadata": {},
   "source": [
    "## Sharing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e1af980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Replace with the actual path to your fine-tuned model\n",
    "model_path = \"Fine_Tuned_Models\"  \n",
    "\n",
    "# We get our model\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fa696c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1a56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.push_to_hub(\"distilbert-base-multilingual-cased-sentiments-student-fine-tuned-data-camp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
